{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhucheng/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning:\n",
      "\n",
      "The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "\n",
      "/Users/zhucheng/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning:\n",
      "\n",
      "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "import dash\n",
    "import plotly.graph_objs as go\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TRAINING_ITER = 20\n",
    "NUM_TREE = 10000\n",
    "TREE_MAX_DEPTH = 12\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(FilePath, delimiter = ',', ):\n",
    "    ''' Read the csv file from FilePath, delimiter of columns ',' '''\n",
    "    presidential = pd.read_csv(FilePath, delimiter = delimiter)\n",
    "    \n",
    "    return presidential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(presidential):\n",
    "    ''' separate features and labels, democratic party labels: column q6, republican party labels: column q7'''\n",
    "    \n",
    "    df = presidential[['greek', 'athlete', 'financialAid',\n",
    "       'gender', 'geography', 'highschool', 'legacy', 'major', 'orientation',\n",
    "       'race', 'year', 'school', 'q5', 'q3', 'q1', 'q7', 'q4', 'q2', 'q6',\n",
    "       'q8']]\n",
    "    df = df[df['q6'] != 5.0]\n",
    "    df = df[df['q7'] != 5.0]\n",
    "    df = df.dropna(subset = ['major','gender','orientation','geography','q2', 'q3', 'q4', 'q5', 'q7', 'q8'])\n",
    "    \n",
    "    ### Increasing values represent higher favorability\n",
    "    df['q6'].replace([1.0, 2.0, 3.0, 4.0], [18.0, 17.0, 16.0, 15.0], inplace=True)\n",
    "    df['q6'].replace([15.0, 16.0, 17.0, 18.0], [1.0, 2.0, 3.0, 4.0], inplace=True)\n",
    "    df['q7'].replace([1.0, 2.0, 3.0, 4.0], [18.0, 17.0, 16.0, 15.0], inplace=True)\n",
    "    df['q7'].replace([15.0, 16.0, 17.0, 18.0], [1.0, 2.0, 3.0, 4.0], inplace=True)\n",
    "    \n",
    "    y_demo = df[['q6']]\n",
    "    y_repu = df[['q7']]\n",
    "    X_features = df[['major','gender','orientation','geography','q2', 'q3', 'q4', 'q5', 'q8']]\n",
    "    \n",
    "    feature_names_value = X_features.columns.tolist()\n",
    "    feature_names_key = range(len(feature_names_value))\n",
    "    \n",
    "    return X_features, y_demo, y_repu, dict(zip(feature_names_key, feature_names_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_dict(X_features):\n",
    "    ''' Return \n",
    "        (1) total number of unique features\n",
    "        (2) feature number dictionary; key - feature(column) name, value - unique number of feature values\n",
    "        (3) feature to order dictionary\n",
    "        (4) order to feature dictionary\n",
    "    '''\n",
    "    featurelist = X_features.columns.tolist()\n",
    "    sum_feature = 0\n",
    "    feature_number_dict = {}\n",
    "    feature_to_order_dict = {}\n",
    "    order_to_feature_dict = {}\n",
    "    for column in featurelist:\n",
    "        # count how many unique values this feature has\n",
    "        column_ft_number = len(X_features[column].unique())\n",
    "        \n",
    "        # add the number of unique feature values to the total feature count\n",
    "        sum_feature += column_ft_number\n",
    "        \n",
    "        # create an entry for this feature, key = feature name, value = number of the unique feature values\n",
    "        feature_number_dict[column] = column_ft_number\n",
    "        \n",
    "        a = list(X_features[column].unique().astype(int))\n",
    "        b = range(column_ft_number)\n",
    "        # map feature # to order\n",
    "        feature_to_order_dict[column] = dict(zip(a, b))\n",
    "        # map order to feature #\n",
    "        order_to_feature_dict[column] = dict(zip(b, a))\n",
    "    \n",
    "    return sum_feature, feature_number_dict, feature_to_order_dict, order_to_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(X_features):\n",
    "    ''' One Hot encode features to prepare for the SMOTE oversampling '''\n",
    "    \n",
    "    sum_feature, feature_number_dict, feature_to_order_dict, order_to_feature_dict = feature_dict(X_features)\n",
    "    onehot = np.array([None for i in range(len(X_features))])\n",
    "    onehot = np.reshape(onehot, (len(X_features),-1))\n",
    "    featurelist = X_features.columns.tolist()\n",
    "    for column in featurelist:\n",
    "        X_column = X_features[column]\n",
    "        X_column = X_column.astype(int)\n",
    "        k = feature_number_dict[column]\n",
    "        \n",
    "        X_onehot = [[0 for j in range(k)] for i in range(len(X_column))]\n",
    "        for count, ele in enumerate(X_column):\n",
    "            b = feature_to_order_dict[column][ele]\n",
    "            X_onehot[count][b-1] = 1\n",
    "        onehot = np.concatenate((onehot, X_onehot), axis=1)\n",
    "    return onehot[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(onehot, ylabel):\n",
    "    ''' SMOTE oversampling '''\n",
    "    \n",
    "    sm = SMOTE()\n",
    "    X_smote, y_smote = sm.fit_sample(onehot, ylabel)\n",
    "    return X_smote, y_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_decode(X_smote, order_to_feature_dict, feature_number_dict):\n",
    "    ''' Decode One Hot features after the SMOTE oversampling back to the original feature format '''\n",
    "    \n",
    "    onehot_de = np.array([None for i in range(len(X_smote))])\n",
    "    onehot_de = np.reshape(onehot_de, (len(X_smote),-1))\n",
    "        \n",
    "    # sum_feature, feature_number_dict, feature_to_order_dict, order_to_feature_dict = feature_dict(X_features)\n",
    "    for key, value in feature_number_dict.items():\n",
    "        X = X_smote[:, :value]\n",
    "        X_smote = X_smote[:, value:]\n",
    "        \n",
    "        # find the column index with the largest value \n",
    "        to_process = np.argmax(X, axis=1)\n",
    "        X_res = []\n",
    "        for item in to_process:\n",
    "            X_res.append(order_to_feature_dict[key][item])\n",
    "        onehot_de = np.concatenate((onehot_de, np.array([X_res]).T), axis=1)\n",
    "    \n",
    "    return onehot_de[:, 1:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_de(X_smote, feature_number_dict):\n",
    "    ''' Decode One Hot features after the SMOTE oversampling back to the original feature format '''\n",
    "    \n",
    "    smote_de = np.array([None for i in range(len(X_smote))])\n",
    "    smote_de = np.reshape(smote_de, (len(X_smote),-1))\n",
    "    m,n = X_smote.shape\n",
    "    smote_res = [[0 for j in range(n)] for i in range(m)]\n",
    "    \n",
    "    # sum_feature, feature_number_dict, feature_to_order_dict, order_to_feature_dict = feature_dict(X_features)\n",
    "    for key, value in feature_number_dict.items():\n",
    "        X = X_smote[:, :value]\n",
    "        X_smote = X_smote[:, value:]\n",
    "        \n",
    "        # find the column index with the largest value \n",
    "        to_process = np.argmax(X, axis=1)\n",
    "        # X_res = []\n",
    "        for i, item in enumerate(to_process):\n",
    "            smote_res[i][item] = 1\n",
    "        \n",
    "    return smote_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train_val set and test set from all the original data\n",
    "def train_val_split(X_features, y_demo, y_repu):\n",
    "    ''' Split training and validation sets \n",
    "        Return\n",
    "        (1) features - X_train, X_val\n",
    "        (2) labels - y_train_demo, y_val_demo (labels - democratic party favorability)\n",
    "                     y_train_repu, y_val_demo (labels - republican party favorability)\n",
    "    '''\n",
    "    \n",
    "    X_train, X_val, y_train_demo, y_val_demo = train_test_split(X_features, y_demo, test_size=0.15, random_state = 24)\n",
    "    y_train_repu = y_repu.loc[y_train_demo.index]\n",
    "    y_val_repu = y_repu.loc[y_val_demo.index]\n",
    "    \n",
    "    X_train = X_train.astype(int)\n",
    "    X_val = X_val.astype(int)\n",
    "    y_train_demo = y_train_demo.astype(int)\n",
    "    y_val_demo = y_val_demo.astype(int)\n",
    "    y_train_repu = y_train_repu.astype(int)\n",
    "    y_val_repu = y_val_repu.astype(int)    \n",
    "    \n",
    "    return X_train, X_val, y_train_demo, y_val_demo, y_train_repu, y_val_repu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier for feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "def rfc_train(X_train, y_train):\n",
    "    ''' Training '''\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = NUM_TREE, max_depth = TREE_MAX_DEPTH,\n",
    "                                 random_state = 33, criterion = 'entropy')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_pred(X_train, y_train, X_val, y_val, classifier):\n",
    "    ''' Prediction \n",
    "        Return \n",
    "        (1) prediction on training set - for training error\n",
    "        (2) prediction on validation set - for validation error\n",
    "    '''\n",
    " \n",
    "    # prediction on training set\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    acc_train = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # prediction on validation set\n",
    "    y_val_pred = classifier.predict(X_val)\n",
    "    cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "    acc_val = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    return y_train_pred, cm_train, acc_train, y_val_pred, cm_val, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    '''\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(classifier, X_features, feature_names):\n",
    "    ''' This function plots the feature importance. '''\n",
    "    \n",
    "    importances = classifier.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in classifier.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    features = []\n",
    "    for index in indices:\n",
    "        features.append(feature_names[index])\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X_features.shape[1]), importances[indices],\n",
    "           color=\"r\", yerr=std[indices]/np.sqrt(NUM_TREE), align=\"center\")\n",
    "    plt.xticks(range(X_features.shape[1]), features, rotation = 30)\n",
    "    plt.xlim([-1, X_features.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_use(X_train_demo, y_train_demo, X_train_repu, y_train_repu, feature_names):\n",
    "    ''' Random forest training, pickle the models'''\n",
    "    \n",
    "    rfc_demo = rfc_train(X_train_demo, y_train_demo)\n",
    "    rfc_repu = rfc_train(X_train_repu, y_train_repu)\n",
    "    filename_demo = 'rfc_demo_model.sav'\n",
    "    pickle.dump(rfc_demo, open(filename_demo, 'wb'))\n",
    "    filename_repu = 'rfc_repu_model.sav'\n",
    "    pickle.dump(rfc_repu, open(filename_repu, 'wb'))\n",
    "    \n",
    "    plot_feature_importance(rfc_demo, X_train_demo, feature_names)\n",
    "    plot_feature_importance(rfc_repu, X_train_repu, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_pred_2(X_train_demo, y_train_demo, X_train_repu, y_train_repu, X_val_demo, y_val_demo, X_val_repu, y_val_repu, classifier_demo, classifier_repu):\n",
    "    ''' Random forest predicting '''\n",
    "    \n",
    "    y_train_pred, cm_train, acc_train, y_val_pred, cm_val, acc_val \\\n",
    "    = rfc_pred(X_train_demo, y_train_demo, X_val_demo, y_val_demo, classifier_demo)\n",
    "    \n",
    "    y_train_pred2, cm_train2, acc_train2, y_val_pred2, cm_val2, acc_val2 \\\n",
    "    = rfc_pred(X_train_repu, y_train_repu, X_val_repu, y_val_repu, classifier_repu)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_confusion_matrix(y_val_demo, y_val_pred, \n",
    "                          title='Confusion matrix, without normalization (democrats)')\n",
    "    plot_confusion_matrix(y_val_repu, y_val_pred2, \n",
    "                          title='Confusion matrix, without normalization (republicans)')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def rfc(presidential):\n",
    "    X_features, y_demo, y_repu, feature_names = data_cleaning(presidential)\n",
    "    X_train_val, X_test, y_train_val_demo, y_test_demo = train_test_split(X_features, y_demo, test_size = 0.15, random_state = 25)\n",
    "    trvaindex = y_train_val_demo.index\n",
    "    testindex = y_test_demo.index\n",
    "    y_train_val_repu = y_repu.loc[trvaindex]\n",
    "    y_test_repu = y_repu.loc[testindex]    \n",
    "    \n",
    "    onehot_enc = onehot_encode(X_features)    \n",
    "    X_smote_demo, y_smote_demo = smote(onehot_enc, np.ravel(y_demo))\n",
    "    X_smote_repu, y_smote_repu = smote(onehot_enc, np.ravel(y_repu))\n",
    "    sum_feature, feature_number_dict, feature_to_order_dict, order_to_feature_dict = feature_dict(X_features)\n",
    "\n",
    "    # Train and test random forest model\n",
    "    X_demo = onehot_decode(X_smote_demo, order_to_feature_dict, feature_number_dict)\n",
    "    X_repu = onehot_decode(X_smote_repu, order_to_feature_dict, feature_number_dict)\n",
    "    \n",
    "    X_train_demo, X_val_demo, y_train_demo, y_val_demo \\\n",
    "    = train_test_split(X_demo, y_smote_demo, test_size=0.18, random_state = 41)\n",
    "    X_train_repu, X_val_repu, y_train_repu, y_val_repu \\\n",
    "    = train_test_split(X_repu, y_smote_repu, test_size=0.18, random_state = 42)\n",
    "    \n",
    "    rfc_use(X_train_demo, y_train_demo, X_train_repu, y_train_repu, feature_names)\n",
    "    \n",
    "    # load the models from drive\n",
    "    filename_demo = 'rfc_demo_model.sav'\n",
    "    rfc_demo = pickle.load(open(filename_demo, 'rb'))\n",
    "    filename_repu = 'rfc_repu_model.sav'\n",
    "    rfc_repu = pickle.load(open(filename_repu, 'rb'))\n",
    "    \n",
    "    # perform random forest prediction using the pickled models\n",
    "    rfc_pred_2(X_train_demo, y_train_demo, X_train_repu, y_train_repu, X_val_demo, y_val_demo, X_val_repu, y_val_repu, rfc_demo, rfc_repu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExactGPModel_train(X_train, y_train):\n",
    "    train_x = torch.Tensor(X_train.astype(int))\n",
    "    train_y = torch.Tensor(y_train.astype(int))\n",
    "\n",
    "    # initialize likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "    ], lr=0.1)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iter = TRAINING_ITER\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            model.likelihood.noise.item()\n",
    "        ))\n",
    "        optimizer.step()\n",
    "    return likelihood, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExactGPModel_test(likelihood, model, X_val):\n",
    "    test_x = torch.Tensor(X_val.astype(int))\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        test_x = torch.Tensor(X_val.astype(int))\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "    return observed_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpr_prediction(y_true, observed_pred, toview):\n",
    "    with torch.no_grad():\n",
    "        # Initialize plot\n",
    "        f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "        # Get upper and lower confidence bounds\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "\n",
    "        toview = toview\n",
    "        # Plot training data as black stars\n",
    "        ax.plot(toview, y_true[toview], 'k')\n",
    "        # Plot predictive means as blue line\n",
    "        # ax.plot(toview, observed_pred.mean.numpy()[toview], 'b')\n",
    "        ax.plot(toview, observed_pred.mean.numpy()[toview], 'b')\n",
    "        # Shade between the lower and upper confidence bounds\n",
    "        ax.fill_between(toview, lower.numpy()[toview], upper.numpy()[toview], alpha=0.5)\n",
    "        ax.set_ylim([-1, 6])\n",
    "        ax.legend(['Observed Data', 'Mean of Prediction', 'Confidence of Prediction'])\n",
    "        ax.set_xlabel('Student ID')\n",
    "        ax.set_ylabel('Party Favorability')\n",
    "\n",
    "        mse = mse_metric(y_true[toview], observed_pred.mean.numpy()[toview])\n",
    "        print('Mean squared error is:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_metric(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpr_histogram(y_true, y_true2, observed_pred, observed_pred2, toview):\n",
    "    toview_range = toview\n",
    "    fig, axs = plt.subplots(1,len(toview_range), figsize = (10,2.5))\n",
    "    \n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    lower2, upper2 = observed_pred2.confidence_region()\n",
    "    \n",
    "    for j,i in enumerate(toview_range):\n",
    "        mu_d = observed_pred.mean.numpy()[i]\n",
    "        #variance_d = np.sqrt(sigma_test_d[toview_range[i]])\n",
    "        variance_d = (lower[i] + upper[i])/2\n",
    "        mu_r = observed_pred2.mean.numpy()[i]\n",
    "        # variance_r = np.sqrt(sigma_test_r[toview_range[i]])\n",
    "        variance_r = (lower2[i] + upper2[i])/2\n",
    "\n",
    "        x = np.linspace(0.5, 4.5, 100)\n",
    "        axs[j].plot(x, scipy.stats.norm.pdf(x, mu_d, variance_d), color = 'blue')\n",
    "        axs[j].plot(x, scipy.stats.norm.pdf(x, mu_r, variance_r), color = 'red')\n",
    "        axs[j].set_xticks([1,2,3,4])\n",
    "        axs[j].axvline(x = y_true[i], color = 'blue', dashes = (3,3,3,3))\n",
    "        axs[j].axvline(x = y_true2[i], color = 'red', dashes = (3,3,3,3))\n",
    "        # axs[i - toview_range[0]].set_title('Student ID '+str(i))\n",
    "        axs[j].set_ylim([0, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpr_train(X_train, y_train, modelname):\n",
    "    gpr_demo = gpr_train(X_train_demo, y_train_demo)\n",
    "    filename_demo = 'rfc_' + modelname + 'demo_gpr.sav'\n",
    "    pickle.dump(gpr_demo, open(filename_demo, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpr_pred(X_val, y_val, model):\n",
    "    gpr_pred, sigma_pred = model.predict(X_val, return_std = True)\n",
    "    plot_gpr(y_val, gpr_pred, sigma_pred, range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpr_train(X_train, y_train):\n",
    "    kernel = DotProduct() + WhiteKernel()\n",
    "    # kernel = RBF()\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel,random_state=20).fit(X_train, y_train)\n",
    "    return gpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpr_pred(X_val, gpr):\n",
    "    gpr_pred, sigma_pred = gpr.predict(X_val, return_std=True)\n",
    "    return gpr_pred, sigma_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpr(y_true, gpr_pred, sigma_pred, toview):\n",
    "    plt.figure()\n",
    "    toview = toview\n",
    "    plt.plot(toview, gpr_pred[toview], c = 'k', label='Prediction')\n",
    "    plt.plot(toview, y_true[toview], c = 'r', label='True Labels')\n",
    "\n",
    "    plt.fill(np.concatenate([toview, toview[::-1]]),\n",
    "             np.concatenate([gpr_pred[toview] - sigma_pred[toview],\n",
    "                            (gpr_pred[toview] + sigma_pred[toview])[::-1]]),\n",
    "             alpha=.5, fc='g', ec='None', label='95% confidence interval')\n",
    "    plt.xlabel('Student ID')\n",
    "    plt.ylabel('Party Favorability')\n",
    "    plt.ylim(-1, 7)\n",
    "    plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpr(presidential):\n",
    "    X_features, y_demo, y_repu, feature_names = data_cleaning(presidential)\n",
    "    X_train_val, X_test, y_train_val_demo, y_test_demo = train_test_split(X_features, y_demo, test_size = 0.15, random_state = 25)\n",
    "    trvaindex = y_train_val_demo.index\n",
    "    testindex = y_test_demo.index\n",
    "    y_train_val_repu = y_repu.loc[trvaindex]\n",
    "    y_test_repu = y_repu.loc[testindex]\n",
    "    \n",
    "    \n",
    "    onehot_enc = onehot_encode(X_train_val)    \n",
    "    X_smote_demo, y_smote_demo = smote(onehot_enc, np.ravel(y_train_val_demo))\n",
    "    X_smote_repu, y_smote_repu = smote(onehot_enc, np.ravel(y_train_val_repu))\n",
    "    sum_feature, feature_number_dict, feature_to_order_dict, order_to_feature_dict = feature_dict(X_train_val)\n",
    "    X_smote_demo = np.array(smote_de(X_smote_demo, feature_number_dict))\n",
    "    X_smote_repu = np.array(smote_de(X_smote_repu, feature_number_dict))\n",
    "\n",
    "    # continued\n",
    "    X_train_demo, X_val_demo, y_train_demo, y_val_demo \\\n",
    "    = train_test_split(X_smote_demo, y_smote_demo, test_size=0.18, random_state = 41)\n",
    "    X_train_repu, X_val_repu, y_train_repu, y_val_repu \\\n",
    "    = train_test_split(X_smote_repu, y_smote_repu, test_size=0.18, random_state = 42)\n",
    "\n",
    "    # Train and test gaussian process regression model\n",
    "    gpr_use(X_train_demo, y_train_demo, X_val_demo, y_val_demo, X_train_repu, y_train_repu, X_val_repu, y_val_repu)\n",
    "\n",
    "    gpr_demo = gpr_train(X_train_demo, y_train_demo)\n",
    "    filename_demo = 'demo_gpr.sav'\n",
    "    pickle.dump(gpr_demo, open(filename_demo, 'wb'))\n",
    "    \n",
    "    gpr_repu = gpr_train(X_train_repu, y_train_repu)\n",
    "    filename_repu = 'repu_gpr.sav'\n",
    "    pickle.dump(gpr_repu, open(filename_repu, 'wb'))\n",
    "    \n",
    "    gpr_demo = pickle.load(open(filename_demo, 'rb'))\n",
    "    gpr_pred_demo, sigma_pred_demo = gpr_pred(X_val_demo, gpr_demo)\n",
    "    \n",
    "    gpr_repu = pickle.load(open(filename_repu, 'rb'))\n",
    "    gpr_pred_repu, sigma_pred_repu = gpr_pred(X_val_repu, gpr_repu)\n",
    "    \n",
    "    toview = range(100)\n",
    "    plot_gpr(y_val_demo, gpr_pred_demo, sigma_pred_demo, toview)\n",
    "    plot_gpr(y_val_repu, gpr_pred_repu, sigma_pred_demo, toview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpr_use(X_train_demo, y_train_demo, X_val_demo, y_val_demo, X_train_repu, y_train_repu, X_val_repu, y_val_repu):\n",
    "    ''' Train the GPR model and test on validation set, save the model '''\n",
    "    \n",
    "    likelihood, model = ExactGPModel_train(X_train_demo, y_train_demo)\n",
    "    observed_pred = ExactGPModel_test(likelihood, model, X_val_demo)\n",
    "    plot_gpr_prediction(y_val_demo, observed_pred, range(100))\n",
    "    torch.save(model.state_dict(), 'model_state.pth')\n",
    "    \n",
    "    likelihood2, model2 = ExactGPModel_train(X_train_repu, y_train_repu)\n",
    "    observed_pred2 = ExactGPModel_test(likelihood2, model2, X_val_repu)\n",
    "    plot_gpr_prediction(y_val_repu, observed_pred2, range(100))\n",
    "    torch.save(model2.state_dict(), 'model2_state.pth')\n",
    "\n",
    "    plot_gpr_histogram(y_val_demo, y_val_repu, observed_pred, observed_pred2, [20,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    presidential = pd.read_csv(\"2020_presidential_tracker.csv\", delimiter = ',')\n",
    "    gpr(presidential)\n",
    "    rfc(presidential)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
